# Week 9 – Reflection

## 1. What the metrics can actually tell me

In Week 9 I was forced to pin down what I mean by “usable”.  
I can’t just say “it feels OK” any more – I have to say **what** I’m measuring:

- Completion for each evaluation task (filter, edit, delete) as 0/1  
- Server-measured time-on-task in milliseconds  
- Separate logging for validation errors and server errors  
- Accessibility confirmations (keyboard-only possible, status messages, focus behaviour)  
- Two UMUX-Lite questions plus a 1–5 confidence rating after each task  

From the small test runs I’ve done, I think **completion + time-on-task** will be the most useful pair:  
if people can’t finish a task at all, or they finish but take much longer than others, that points to a real design problem.  
Then I can use confidence and UMUX-Lite scores to explain *why* the numbers look like that.

---

## 2. Why I deliberately kept the metrics simple

At the beginning I wanted to log lots of fancy things like click counts or scroll distance.  
After thinking about it, in a **server-first + HTMX / no-JS dual-path** system it is actually quite hard to collect those reliably, and they might not be worth the engineering cost for this prototype.

So I deliberately kept only:

- **Simple, robust metrics** that work for both HTMX and no-JS paths  
- Data that maps directly onto the three evaluation tasks (T1_filter, T2_edit, T3_delete)  
- Things I can realistically calculate and explain with a small peer sample in Week 10  

For time-on-task I decided to use the **median** instead of the mean, because there will definitely be outliers:  
people get interrupted, or get stuck on some detail for a long time. The median is closer to “most people’s experience”.

---

## 3. Ethics and privacy considerations

When I wrote the protocol I spent more time than usual thinking about privacy.  
Some key decisions were:

- Use the random `session_id` generated by `SessionUtils`, *not* names or student numbers  
- **Never** store obvious PII in the logs (no email, no IP)  
- Keep logs only in a private repo, and let participants use their session ID to request deletion  
- Keep the whole study “low-risk”: short peer-to-peer tests, no audio/video recording  

After writing out the full consent script, I felt more confident about how to explain  
“what we collect and why” to participants. It also reminded me that:

> “You can stop at any time” has to be **true**, not just a nice sentence in the protocol.

---

## 4. Trade-offs in instrumentation

I made the server-side instrumentation intentionally “thin”.  
Right now the logger tells me:

- Which evaluation task this request belongs to (T1, T2, T3)  
- Whether it ended in success, validation error, or server error  
- How many milliseconds this attempt took on the server  
- Whether it was an HTMX request (`js_mode=on`) or a no-JS path (`js_mode=off`)  

But it cannot see details like browser rendering time, how many times the user scrolled,  
or how many times they edited the same field – those have to come from my observation notes.

For this **server-first prototype** I think that trade-off is acceptable:  
the logs stay simple and reliable, and they are privacy-friendly.  
I can then combine them with qualitative notes during analysis.

---

## 5. How realistic the tasks are

The three tasks I defined are intentionally small, but I tried to keep them realistic:

- **T1_filter** – filter by a keyword and report how many tasks remain  
- **T2_edit** – change “draft report” to “submit report”  
- **T3_delete** – delete a temporary task called “tmp”  

These map onto what a student might actually do: find tasks, correct tasks, and delete tasks.  
If I later recruit non-CS participants, I might rewrite the scenarios with more everyday language  
and more context, but the pattern of **one search task + one correction task + one destructive task**  
should still generalise.

---

## 6. How ready I feel for Week 9 Lab 2

After this lab I feel much better prepared for the peer pilots in Week 9 Lab 2:

- I have a clear task script and rough time limits (around 3 minutes per task)  
- I know exactly which metrics I’ll record and where they are stored  
  (metrics log + handwritten notes)  
- The protocol explains consent, anonymity, and opt-out in straightforward language  
- I already have basic plans for variants like keyboard-only and no-JS sessions  

The biggest risks I can see are:

- **Technical** – for example, mis-configured logging or forgetting to attach the `session_id`  
- **Operational** – for example, forgetting to ask for the confidence rating or to write down observations  

To reduce this, I plan to run at least one dry run on myself (or with a friend) before the real pilots,  
so I can practise the flow once and catch any obvious mistakes.

Overall, Week 9 shifted my mindset from “does this interface look OK?”  
to “what evidence do I have that people can actually use it, and how will I collect that evidence?”.
